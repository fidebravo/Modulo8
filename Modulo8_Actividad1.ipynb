{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 8 Actividad 1\n",
    "\n",
    "## Intención del aprendizaje esperado:\n",
    "\n",
    "**1. Describir las características fundamentales de Big Data y su ecosistema para el manejo de grandes volúmenes de datos.**\n",
    "\n",
    "## Ejercicios Planteados\n",
    "\n",
    "**Responda lo siguiente:**\n",
    "\n",
    "*• ¿Qué es Tensorflow?*\n",
    "\n",
    "*• ¿Qué es Hadoop?*\n",
    "\n",
    "*• ¿Cómo funciona Hadoop?*\n",
    "\n",
    "*• ¿Qué es una API?*\n",
    "\n",
    "*• ¿Qué es Spark?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow** es una librería de código abierto para Machine Learning desarrollada por *Google Brain* y que se utiliza para construir y entrenar modelos de aprendizaje automático y redes neuronales para detectar patrones y razonamientos que imiten el comportamiento humano.\n",
    "\n",
    "**Hadoop** es un framework de código abierto utilizado para almacenar distribuidamente y procesar grandes cantidades de datos (del oreden de Gygabytes o Petabytes) en clúster de computadores.  Su estructura permite la escalabilidad vertical lo que beneficia el análisis de grandes conjuntos de datos en paralelo y con mayor rapidez.  Es una de las plataformas más populares para el desarrollo de Big Data.\n",
    "\n",
    "- Hadoop Distributed File System *(HDFS)* es el mecanismo con que Hadoop hace la distribución de los datos en nodos de bloques de tamaño fijo.  Una característica es que estos bloques se repiten en varios nodos de tal manera de garantizar la redundancia y tolerancia al fallos, de manera que si un nodo falla aun la información puede ser consultada en otros nodos.\n",
    "\n",
    "- MapReduce, por otra parte, es el segundo componente del nucleo de Hadoop y corresponde a un modelo de programación y procesamiento de grandes cantidades de datos de manera distribuida en clúster de computadores.  Actúa en general con dos etapas:  Mapeo  de datos, que es donde se procesan y filtran los datos que se quiere analizar; la etapa de reducción, donde se agregan y resumen los resultados intermedios que serán usados para producir el resultado final.\n",
    "\n",
    "**API** es el acrónimo de *Application Programming Interface (Interfaz de Programación de Aplicaciones en castellano)* y consiste en una serie de reglas y protocolos que permiten a distintos software comunicarse entre sí.  Analogando al mundo real, una API es como un interprete traductor del mundo digital que permite que distintas Aplicaciones puedan interactuar y transferir información según los requerimientos.\n",
    "\n",
    "**Spark** o Apache Spark es un framework de procesamiento de grandes cantidades de datos de manera distribuida y ultrarrápida en comparación con otras alternativas como Hadoop.  Una de las ventajes es, efectivamente, su velocidad y costo computacional ya que el análisis que realiza lo hace usando memoria RAM de clúster de computadoras lo que permite que el proceso sea más rápido que si la información estuviera en disco duro como también que la cantidad de nodos a utilizar sea mucho menor en órdenes de magnitud"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
